{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9982c524",
   "metadata": {},
   "source": [
    "# Recuperative Practice - HarryPotter Finding her GodFather Sirius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8445eab",
   "metadata": {},
   "source": [
    "The situacion is:\n",
    "Harry Potter wants to rescue his godfather named Sirius from Azkaban prison, but for this you have to cross a road that has strong winds.\n",
    "\n",
    "<img src=\"mapa.jpg\">\n",
    "\n",
    "This is the map that he has to cross. There are three columns with strong winds, the column A and C, will push Harry one row, and the column B will push two rows.\n",
    "The map is \"7 x 9\" of size, and the odd of been pushed for the wind are: Column A = 0.2; Column B = 0.3; Column C = 0.25 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3400c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71e4f5",
   "metadata": {},
   "source": [
    "First at all, i import some libraries that they will give me some functions and tools to create the scenario.<br>\n",
    "\"numpy\" is helping to create a matrix, finding the max argument in an array, and to have the exponent of a number.<br>\n",
    "\"randon\" to get a random number, to get the possibility if the wind will push to Harry.<br>\n",
    "\"time\" to get some breaks trying to simulate the scenario.<br>\n",
    "\"clear_output\" to clear the  results of a code block.<br>\n",
    "\"pyplot\" to get a grafic for the rewards and see how the program trained <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47136ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "#Actions\n",
    "action_space_size = 4\n",
    "#Total spaces\n",
    "state_space_size = 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1356a",
   "metadata": {},
   "source": [
    "\"gym\" is a librery that import a full situacion for a frozen map, with all the functions to draw the map, get actions, get a initial state, a reset all. But for this practice, i want a diferent situation, so i research the functions that it use, and re-write them here for this situation. But it will be the reference for the practice.\n",
    "\n",
    "First at all, we set the number of the actions, and the number of states in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26236cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of the episodes to training\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.2\n",
    "discount_rate = 0.95\n",
    "\n",
    "rewards_avg = []\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02731d",
   "metadata": {},
   "source": [
    "Then i set the number of episodes that the training will be simulate, and the max steps per episode to have a limit for every training.\n",
    "\n",
    "Then i set a learning rate and a discount rate, to have a rate for each action in each state. And see what action the simulation will take.\n",
    "\n",
    "It creates a matrix that will have a space for each action for each state. So in this case, it will create a \"63 x 4\" matrix. To asign a rate for each action in each state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715c1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12. 13. 14. 15. 16. 17.]\n",
      " [18. 19. 20. 21. 22. 23. 24. 25. 26.]\n",
      " [27. 28. 29. 30. 31. 32. 33. 34. 35.]\n",
      " [36. 37. 38. 39. 40. 41. 42. 43. 44.]\n",
      " [45. 46. 47. 48. 49. 50. 51. 52. 53.]\n",
      " [54. 55. 56. 57. 58. 59. 60. 61. 62.]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "mapa = np.zeros((7,9))\n",
    "varX = 1\n",
    "varY = 1\n",
    "for i in range(7):\n",
    "    for j in range(9):\n",
    "        #print(varX, varY)\n",
    "        mapa[i][j] = varY - 1\n",
    "        varY +=1\n",
    "    varX +=1\n",
    "print(mapa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4ed38",
   "metadata": {},
   "source": [
    "This will be the reference map, to have an idea how the \"q_table\" will work, each state has an id, and each id has 4 actions with a rate. So it needs config how will be the moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66f406eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "13\n",
      "32\n",
      "32\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "prob_wind_A = 0.2\n",
    "prob_wind_B = 0.3\n",
    "prob_wind_C = 0.25\n",
    "\n",
    "#Moves: 0 left; 3 up; 2 right; 1 down \n",
    "def getNewState(state, action):\n",
    "    newState = state\n",
    "    #left\n",
    "    if action == 0:\n",
    "        if state % 9 == 0:\n",
    "            newState = state\n",
    "        else:\n",
    "            newState = state - 1\n",
    "    #up\n",
    "    if action == 3:\n",
    "        if state >= 0 and state < 9:\n",
    "            newState = state\n",
    "        else:\n",
    "            newState = state - 9\n",
    "    #right\n",
    "    if action == 2:\n",
    "        if state % 9 == 8:\n",
    "            newState = state\n",
    "        else:\n",
    "            newState = state + 1\n",
    "    #down\n",
    "    if action == 1:\n",
    "        if state > 53 and state < 63:\n",
    "            newState = state\n",
    "        else:\n",
    "            newState = state + 9\n",
    "    #wind\n",
    "    #column A\n",
    "    if newState % 9 == 3:\n",
    "        if newState > 3:\n",
    "            thrust_rate_threshold = random.uniform(0, 1)\n",
    "            if thrust_rate_threshold <= prob_wind_A:\n",
    "                newState -= 9 \n",
    "    #column B\n",
    "    if newState % 9 == 4:\n",
    "        if newState > 4:\n",
    "            thrust_rate_threshold = random.uniform(0, 1)\n",
    "            if thrust_rate_threshold <= prob_wind_B:\n",
    "                if newState == 13:\n",
    "                    newState -= 9\n",
    "                else:\n",
    "                    newState -= 18\n",
    "    #column C\n",
    "    if newState % 9 == 5:\n",
    "        if newState > 5:\n",
    "            thrust_rate_threshold = random.uniform(0, 1)\n",
    "            if thrust_rate_threshold <= prob_wind_C:\n",
    "                newState -= 9 \n",
    "    return newState\n",
    "\n",
    "print(getNewState(4,3))\n",
    "print(getNewState(4,1))\n",
    "print(getNewState(23,1))\n",
    "print(getNewState(32,4))\n",
    "print(getNewState(42,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9c3c0",
   "metadata": {},
   "source": [
    "This is the code key, here we get an future state, sending it a actual state and an action, how i mentioned before, the algorithm takes the states with a number. And it is programmed to return the future state with and actual state and an action, for example:\n",
    "\n",
    "In the last four lines of code, i send the state with id of 4, and the action number 3 and 1.\n",
    "\n",
    "<img src=\"example.jpg\">\n",
    "\n",
    "How the image shows, the code of the moves are the next: \n",
    "Moves: 0 left; 3 up; 2 right; 1 down \n",
    "\n",
    "So if we send it the state number 4, and the action number 1, it will try to get down a box. But takes the probability that the wind will push you back to the same place where you started.\n",
    "\n",
    "The test are for the limits, but Harry cant move a box out of the map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f27401",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8d522",
   "metadata": {},
   "source": [
    "Here is where the training happen, so it will 100 times, with the number of episodes and the max steps per episode.  \n",
    "It has the Inicial state set in 10, so Harry Potter will start in the state with the id of 10, and her godfather in the state 43.\n",
    "So first at all, we have a \"q_table\" full of zeros, so it will explore with all the moves in the states, to asign a rate for each move, using the function mentioned before to get the next state, it can train moving around the map until reaching the goal.\n",
    "The main reward will be a 10 if gets the goal state, and a -1 if it keeps moving around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a7bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average  0\n",
      "average  1\n",
      "average  2\n",
      "average  3\n",
      "average  4\n",
      "average  5\n",
      "average  6\n",
      "average  7\n",
      "average  8\n",
      "average  9\n",
      "average  10\n",
      "average  11\n",
      "average  12\n",
      "average  13\n",
      "average  14\n",
      "average  15\n",
      "average  16\n",
      "average  17\n",
      "average  18\n",
      "average  19\n",
      "average  20\n",
      "average  21\n",
      "average  22\n",
      "average  23\n",
      "average  24\n",
      "average  25\n",
      "average  26\n",
      "average  27\n",
      "average  28\n",
      "average  29\n",
      "average  30\n",
      "average  31\n",
      "average  32\n",
      "average  33\n",
      "average  34\n",
      "average  35\n",
      "average  36\n",
      "average  37\n",
      "average  38\n",
      "average  39\n",
      "average  40\n",
      "average  41\n",
      "average  42\n",
      "average  43\n",
      "average  44\n",
      "average  45\n",
      "average  46\n",
      "average  47\n",
      "average  48\n",
      "average  49\n",
      "average  50\n",
      "average  51\n",
      "average  52\n",
      "average  53\n",
      "average  54\n",
      "average  55\n",
      "average  56\n",
      "average  57\n",
      "average  58\n",
      "average  59\n",
      "average  60\n",
      "average  61\n",
      "average  62\n",
      "average  63\n",
      "average  64\n",
      "average  65\n",
      "average  66\n",
      "average  67\n",
      "average  68\n",
      "average  69\n",
      "average  70\n",
      "average  71\n",
      "average  72\n",
      "average  73\n",
      "average  74\n",
      "average  75\n",
      "average  76\n",
      "average  77\n",
      "average  78\n",
      "average  79\n",
      "average  80\n",
      "average  81\n",
      "average  82\n",
      "average  83\n",
      "average  84\n",
      "average  85\n",
      "average  86\n",
      "average  87\n",
      "average  88\n",
      "average  89\n",
      "average  90\n",
      "average  91\n",
      "average  92\n",
      "average  93\n",
      "average  94\n",
      "average  95\n",
      "average  96\n",
      "average  97\n",
      "average  98\n",
      "average  99\n"
     ]
    }
   ],
   "source": [
    "# This cycle is to calculate the average reward/episodes and its only purpose is to plot the nice graph below that\n",
    "# shows how the agent learn how to maximize the reward.\n",
    "for it in range(100):\n",
    "    print('average ', it)\n",
    "    rewards_all_episodes=[]\n",
    "    \n",
    "    # exporation-exploitation trade-off params\n",
    "    exploration_rate = 1\n",
    "    max_exploration_rate = 1\n",
    "    min_exploration_rate = 0.01\n",
    "    exploration_decay_rate = 0.005\n",
    "    \n",
    "    # init q table in zeros\n",
    "    q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "    # iterate over the episodes\n",
    "    for episode in range(num_episodes):\n",
    "        #state = env.reset()\n",
    "        state = 10\n",
    "        goal = 43\n",
    "        rewards_current_episode = 0\n",
    "        done = False\n",
    "        \n",
    "        # iterate over the steps for an episode\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = np.argmax(q_table[state,:])          \n",
    "            new_state = getNewState(state, action)\n",
    "            if new_state == goal:\n",
    "                reward = 10\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -1\n",
    "            # Update Q-table for Q(s,a)\n",
    "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "            # transition next state\n",
    "            state = new_state \n",
    "            rewards_current_episode += reward\n",
    "            if done = True:\n",
    "                break\n",
    "\n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "    rewards_avg.append(rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde0e5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19ef8208460>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(0,num_episodes)]\n",
    "y = np.mean(rewards_avg, axis=0)\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Reward')\n",
    "plot.plot(x, y,'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded96be",
   "metadata": {},
   "source": [
    "So now, time to graphic how the training was. There is a main value of the reward for each episode, so first we can see that it ends with a negative rate to moving around so much. But it learns so fast, and it needs less than a thousand episodes to reach a optimal reward. So there are more than 80000 episodes that are left over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bdb7762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[-3.14113613e+00 -3.22020507e+00 -3.09125793e+00 -3.14113613e+00]\n",
      " [-3.10534891e+00 -3.11953340e+00 -3.14954483e+00 -3.14113613e+00]\n",
      " [-2.91116297e+00 -2.82103790e+00 -2.87730068e+00 -2.79883291e+00]\n",
      " [-2.57133906e+00 -2.58674015e+00 -1.88445229e+00 -2.62508374e+00]\n",
      " [-2.35076561e+00  5.88116898e+01 -2.26283288e+00 -2.27230257e+00]\n",
      " [-1.86236037e+00 -1.74884550e+00  6.72983622e+01 -1.72965505e+00]\n",
      " [-1.27565786e+00  7.18930128e+01 -1.11940524e+00 -1.17039701e+00]\n",
      " [-9.44948043e-01 -6.89200000e-01 -7.78340000e-01 -7.88079800e-01]\n",
      " [-6.77848433e-01 -5.86800000e-01 -5.94020000e-01 -5.94020000e-01]\n",
      " [-3.14113613e+00 -3.09886345e+00 -3.01150065e+00 -3.14609648e+00]\n",
      " [-3.35118211e+00  4.96855987e+01 -3.24452008e+00 -3.32774268e+00]\n",
      " [-2.68417294e+00 -2.70916949e+00 -2.74252368e+00 -2.78196032e+00]\n",
      " [-2.36928955e+00  5.76056418e+01 -2.12618100e+00 -2.22972280e+00]\n",
      " [-1.41601971e+00 -1.48869684e+00  6.50228609e+01 -1.56724123e+00]\n",
      " [-8.17668784e-01 -8.12156600e-01  7.18930128e+01 -1.05044707e+00]\n",
      " [-8.82901255e-01  7.67294872e+01 -5.63620000e-01 -7.60478240e-01]\n",
      " [-4.05220000e-01 -5.26000000e-01 -5.26000000e-01 -4.58800000e-01]\n",
      " [-4.28400000e-01 -5.26000000e-01 -3.98000000e-01 -3.98000000e-01]\n",
      " [-2.79883291e+00 -2.75063650e+00 -2.80850048e+00 -2.68738967e+00]\n",
      " [-2.63150647e+00 -2.60815869e+00  5.31551853e+01 -2.73679206e+00]\n",
      " [-2.19350537e+00 -2.18050712e+00  5.73916982e+01 -2.35854328e+00]\n",
      " [-1.80663720e+00 -1.69736003e+00  6.36196983e+01 -1.76656382e+00]\n",
      " [-1.19813324e+00 -1.11435770e+00  7.05597060e+01 -1.08129364e+00]\n",
      " [-9.66716797e-01 -7.26212000e-01  7.67294872e+01 -1.02666663e+00]\n",
      " [-4.05220000e-01 -5.26000000e-01  8.18205128e+01 -4.66020000e-01]\n",
      " [-4.05220000e-01  8.71794872e+01 -2.00000000e-01 -2.38000000e-01]\n",
      " [-9.04107929e-02 -2.00000000e-01 -2.00000000e-01 -2.38000000e-01]\n",
      " [-2.44957954e+00 -2.46929673e+00 -2.36416113e+00 -2.49349196e+00]\n",
      " [-2.33156786e+00 -2.21727748e+00 -2.13682879e+00 -2.36118584e+00]\n",
      " [-1.86596820e+00 -1.89483214e+00 -1.91865004e+00 -1.57635414e+00]\n",
      " [-1.50612768e+00 -1.37768578e+00 -3.78691225e-01 -1.35235833e+00]\n",
      " [-9.61840884e-01 -9.23417928e-01 -9.84337680e-01 -1.14179096e+00]\n",
      " [-6.66834840e-01 -5.18400000e-01 -1.05918957e-01 -5.47820594e-01]\n",
      " [-3.98000000e-01  1.55495056e+01 -2.00000000e-01 -2.38000000e-01]\n",
      " [-2.00000000e-01  9.28205128e+01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.80000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.09323491e+00 -2.09856166e+00 -2.14429015e+00 -2.21708544e+00]\n",
      " [-1.93427455e+00 -1.91766448e+00 -1.98009613e+00 -2.07698472e+00]\n",
      " [-1.74294833e+00 -1.63069149e+00 -1.58759740e+00 -1.80464317e+00]\n",
      " [-1.17597337e+00 -1.09023703e+00 -1.13669728e+00 -1.34219234e+00]\n",
      " [-8.92674725e-01 -5.14771171e-01 -6.83120000e-01 -8.95254474e-01]\n",
      " [-4.61327388e-01 -3.60000000e-01  1.55954572e+01 -2.75620000e-01]\n",
      " [-2.00000000e-01 -2.00000000e-01  7.59110094e+01  0.00000000e+00]\n",
      " [-2.00000000e-01 -2.00000000e-01 -2.00000000e-01  8.71794872e+01]\n",
      " [ 2.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.91235850e+00 -1.87982361e+00 -1.81008173e+00 -1.96471692e+00]\n",
      " [-1.62184777e+00 -1.71131503e+00 -1.65086453e+00 -1.76601762e+00]\n",
      " [-1.53745790e+00 -1.41901388e+00 -1.33743619e+00 -1.53256912e+00]\n",
      " [-1.16471898e+00 -1.02097116e+00 -9.26459666e-01 -9.36769316e-01]\n",
      " [-4.87740800e-01 -2.38000000e-01  4.12883229e-01 -2.75620000e-01]\n",
      " [-2.38000000e-01 -3.98000000e-01  3.24213617e+00 -2.38000000e-01]\n",
      " [-2.00000000e-01 -2.00000000e-01 -2.00000000e-01  2.47359647e+01]\n",
      " [-2.00000000e-01 -2.00000000e-01 -2.00000000e-01  1.77965789e+01]\n",
      " [-2.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.72965505e+00 -1.72965505e+00 -1.71937217e+00 -1.85673990e+00]\n",
      " [-1.66373812e+00 -1.54510611e+00 -1.51350311e+00 -1.47705689e+00]\n",
      " [-1.12543383e+00 -1.17039701e+00 -1.23717320e+00 -1.34891221e+00]\n",
      " [-7.34733810e-01 -6.31263800e-01 -7.11684623e-01 -5.10079480e-01]\n",
      " [-2.00000000e-01 -2.38000000e-01 -2.00000000e-01 -2.38000000e-01]\n",
      " [-2.00000000e-01 -2.00000000e-01 -2.00000000e-01 -2.00000000e-01]\n",
      " [-2.00000000e-01 -2.00000000e-01 -2.00000000e-01 -2.00000000e-01]\n",
      " [-2.00000000e-01 -2.00000000e-01 -2.00000000e-01 -2.00000000e-01]\n",
      " [-2.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a11b",
   "metadata": {},
   "source": [
    "Now, we can see how the \"q_table\" ends. It still has some states that doesn't prove all the moves, but it has a better move for each state. So time to simulate the situacion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd8171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "mapa2 = [['S','S','S','W','W','W','S','S','S'],\n",
    " ['S','H','S','W','W','W','S','S','S'],\n",
    " ['S','S','S','W','W','W','S','S','S'],\n",
    " ['S','S','S','W','W','W','S','S','S'],\n",
    " ['S','S','S','W','W','W','S','G','S'],\n",
    " ['S','S','S','W','W','W','S','S','S'],\n",
    " ['S','S','S','W','W','W','S','S','S']]\n",
    "lastAction = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d79ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(state, lastAction, mode=\"human\"):\n",
    "    outfile = StringIO() if mode == \"ansi\" else sys.stdout\n",
    "    row = int(state/9)\n",
    "    col = int(state/9)\n",
    "    mapa2[row][col] = utils.colorize(mapa2[row][col], \"red\", highlight=True)\n",
    "    if lastAction is not None:\n",
    "        outfile.write(f\"  ({['Left', 'Down', 'Right', 'Up'][lastAction]})\\n\")\n",
    "    else:\n",
    "        outfile.write(\"\\n\")\n",
    "    outfile.write(\"\\n\".join(\"\".join(line) for line in mapa2) + \"\\n\")\n",
    "    if mode != \"human\":\n",
    "        with closing(outfile):\n",
    "            return outfile.getvalue()\n",
    "\n",
    "def restartRender():\n",
    "    import sys\n",
    "    from contextlib import closing\n",
    "    import numpy as np\n",
    "    from io import StringIO\n",
    "    from gym import utils\n",
    "    from gym.envs.toy_text import discrete\n",
    "    mapa2 = [['S','S','S','W','W','W','S','S','S'],\n",
    "     ['S','H','S','W','W','W','S','S','S'],\n",
    "     ['S','S','S','W','W','W','S','S','S'],\n",
    "     ['S','S','S','W','W','W','S','S','S'],\n",
    "     ['S','S','S','W','W','W','S','G','S'],\n",
    "     ['S','S','S','W','W','W','S','S','S'],\n",
    "     ['S','S','S','W','W','W','S','S','S']]\n",
    "    lastAction = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adbe9e",
   "metadata": {},
   "source": [
    "First at all, i tried to use the function \"render\" to have a visual simulation, but i had problems to reset the simulation, and it has some troubles for been an external function, but i left the result to see how it would be.\n",
    "I assigned each state a letter, having:\n",
    "S - Suelo\n",
    "H - Harry Potter\n",
    "W - Vientos\n",
    "G - Goal (Sirius)\n",
    "\n",
    "So the simulation will moving in the map until get the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff4e44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SSSWWWSSS\n",
      "S\u001b[41mH\u001b[0mSWWWSSS\n",
      "SSSWWWSSS\n",
      "SSSWWWSSS\n",
      "SSSWWWSGS\n",
      "SSSWWWSSS\n",
      "SSSWWWSSS\n"
     ]
    }
   ],
   "source": [
    "# render(11,None)\n",
    "# render(20,1)\n",
    "# render(21,2)\n",
    "# render(14,2)\n",
    "\n",
    "\n",
    "#render(60,2)\n",
    "\n",
    "restartRender()\n",
    "render(11,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb5893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "\u001b[41mS\u001b[0mSSWWWSSS\n",
      "S\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41mH\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mSWWWSSS\n",
      "SS\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41mS\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mWWWSSS\n",
      "SSS\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41mW\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mWWSSS\n",
      "SSSW\u001b[41m\u001b[41m\u001b[41m\u001b[41m\u001b[41mW\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mWSGS\n",
      "SSSWWWSSS\n",
      "SSSWWWSSS\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "pasos = []\n",
    "for episode in range(5):\n",
    "    restartRender()\n",
    "    state = 10\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    lastAction = None\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        print(pasos) \n",
    "        pasos.append([int(state/9),int(state%9)])\n",
    "        clear_output(wait=True)\n",
    "        render(state,lastAction)\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state = getNewState(state, action)\n",
    "        if new_state == goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "        lastAction = action\n",
    "        state = new_state\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            render(state, lastAction)\n",
    "            print(\"****You reached the goal!****\")\n",
    "            pasos.append([-1,-1])\n",
    "            time.sleep(3)\n",
    "            break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf21d5",
   "metadata": {},
   "source": [
    "This is the prove simulation, but how it shows, it doesn't restart the visual simulation, but it gets the goal, so i saved each step that it takes, to have an idea how Harry will move with the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943834a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [2, 1], [2, 2], [1, 3], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 7], [-1, -1], [1, 1], [2, 1], [2, 2], [2, 3], [2, 4], [1, 5], [1, 6], [2, 6], [2, 7], [3, 7], [-1, -1], [1, 1], [2, 1], [2, 2], [2, 3], [0, 4], [1, 4], [1, 5], [1, 6], [2, 6], [2, 7], [3, 7], [-1, -1], [1, 1], [2, 1], [2, 2], [2, 3], [2, 4], [1, 5], [1, 6], [2, 6], [2, 7], [3, 7], [-1, -1], [1, 1], [2, 1], [2, 2], [1, 3], [1, 3], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 7], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "print(pasos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32ff2a",
   "metadata": {},
   "source": [
    "This are all the steps per episode, it doesn't count the goal step, but the coordenates of the goal state are [4,7]. So if ends with a step of [3,7], [4,6], [4,8] or [5,7]. It always will take the action to move to the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08348b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenada objetivo:  [4, 7]\n",
      "Episodio  1\n",
      "Paso  0 : [1, 1]\n",
      "Paso  1 : [2, 1]\n",
      "Paso  2 : [2, 2]\n",
      "Paso  3 : [1, 3]\n",
      "Paso  4 : [2, 3]\n",
      "Paso  5 : [2, 4]\n",
      "Paso  6 : [2, 5]\n",
      "Paso  7 : [2, 6]\n",
      "Paso  8 : [2, 7]\n",
      "Paso  9 : [3, 7]\n",
      "Episodio  2\n",
      "Paso  0 : [1, 1]\n",
      "Paso  1 : [2, 1]\n",
      "Paso  2 : [2, 2]\n",
      "Paso  3 : [2, 3]\n",
      "Paso  4 : [2, 4]\n",
      "Paso  5 : [1, 5]\n",
      "Paso  6 : [1, 6]\n",
      "Paso  7 : [2, 6]\n",
      "Paso  8 : [2, 7]\n",
      "Paso  9 : [3, 7]\n",
      "Episodio  3\n",
      "Paso  0 : [1, 1]\n",
      "Paso  1 : [2, 1]\n",
      "Paso  2 : [2, 2]\n",
      "Paso  3 : [2, 3]\n",
      "Paso  4 : [2, 4]\n",
      "Paso  5 : [1, 5]\n",
      "Paso  6 : [1, 6]\n",
      "Paso  7 : [2, 6]\n",
      "Paso  8 : [2, 7]\n",
      "Paso  9 : [3, 7]\n",
      "Episodio  4\n",
      "Paso  0 : [1, 1]\n",
      "Paso  1 : [2, 1]\n",
      "Paso  2 : [2, 2]\n",
      "Paso  3 : [2, 3]\n",
      "Paso  4 : [2, 4]\n",
      "Paso  5 : [1, 5]\n",
      "Paso  6 : [1, 6]\n",
      "Paso  7 : [2, 6]\n",
      "Paso  8 : [2, 7]\n",
      "Paso  9 : [3, 7]\n",
      "Episodio  5\n",
      "Paso  0 : [1, 1]\n",
      "Paso  1 : [2, 1]\n",
      "Paso  2 : [2, 2]\n",
      "Paso  3 : [2, 3]\n",
      "Paso  4 : [2, 4]\n",
      "Paso  5 : [1, 5]\n",
      "Paso  6 : [1, 6]\n",
      "Paso  7 : [2, 6]\n",
      "Paso  8 : [2, 7]\n",
      "Paso  9 : [3, 7]\n"
     ]
    }
   ],
   "source": [
    "objetivo = [int(goal/9),int(goal%9)]\n",
    "print(\"Coordenada objetivo: \",objetivo)\n",
    "\n",
    "variable = 0\n",
    "for i in range(5):\n",
    "    print(\"Episodio \",i+1)\n",
    "    boole = False\n",
    "    for j in range(20):\n",
    "        if boole == False:\n",
    "            if(pasos[j+variable] != [-1,-1]):\n",
    "                print(\"Paso \",j,\":\" ,pasos[j+variable])\n",
    "            else:\n",
    "                variable = j + 1\n",
    "                boole = True\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ab0ef",
   "metadata": {},
   "source": [
    "So this are the steps for each episode, the are only one episode that gets push for whe wind, and is the 2nd episode, after he takes the short way to get the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bc6dd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for state  0 : \n",
      "Optimal policy for state  1 : left\n",
      "Optimal policy for state  2 : right\n",
      "Optimal policy for state  3 : \n",
      "Optimal policy for state  4 : down\n",
      "Optimal policy for state  5 : \n",
      "Optimal policy for state  6 : down\n",
      "Optimal policy for state  7 : down\n",
      "Optimal policy for state  8 : down\n",
      "Optimal policy for state  9 : \n",
      "Optimal policy for state  10 : down\n",
      "Optimal policy for state  11 : left\n",
      "Optimal policy for state  12 : down\n",
      "Optimal policy for state  13 : \n",
      "Optimal policy for state  14 : \n",
      "Optimal policy for state  15 : down\n",
      "Optimal policy for state  16 : left\n",
      "Optimal policy for state  17 : \n",
      "Optimal policy for state  18 : right\n",
      "Optimal policy for state  19 : \n",
      "Optimal policy for state  20 : \n",
      "Optimal policy for state  21 : \n",
      "Optimal policy for state  22 : \n",
      "Optimal policy for state  23 : \n",
      "Optimal policy for state  24 : \n",
      "Optimal policy for state  25 : down\n",
      "Optimal policy for state  26 : left\n",
      "Optimal policy for state  27 : \n",
      "Optimal policy for state  28 : \n",
      "Optimal policy for state  29 : right\n",
      "Optimal policy for state  30 : \n",
      "Optimal policy for state  31 : down\n",
      "Optimal policy for state  32 : \n",
      "Optimal policy for state  33 : down\n",
      "Optimal policy for state  34 : down\n",
      "Optimal policy for state  35 : left\n",
      "Optimal policy for state  36 : left\n",
      "Optimal policy for state  37 : down\n",
      "Optimal policy for state  38 : \n",
      "Optimal policy for state  39 : down\n",
      "Optimal policy for state  40 : down\n",
      "Optimal policy for state  41 : \n",
      "Optimal policy for state  42 : \n",
      "Optimal policy for state  43 : right\n",
      "Optimal policy for state  44 : left\n",
      "Optimal policy for state  45 : \n",
      "Optimal policy for state  46 : left\n",
      "Optimal policy for state  47 : \n",
      "Optimal policy for state  48 : \n",
      "Optimal policy for state  49 : \n",
      "Optimal policy for state  50 : \n",
      "Optimal policy for state  51 : right\n",
      "Optimal policy for state  52 : right\n",
      "Optimal policy for state  53 : down\n",
      "Optimal policy for state  54 : \n",
      "Optimal policy for state  55 : right\n",
      "Optimal policy for state  56 : left\n",
      "Optimal policy for state  57 : right\n",
      "Optimal policy for state  58 : left\n",
      "Optimal policy for state  59 : left\n",
      "Optimal policy for state  60 : left\n",
      "Optimal policy for state  61 : left\n",
      "Optimal policy for state  62 : down\n"
     ]
    }
   ],
   "source": [
    "def getBestMoves(num):\n",
    "    res = \"\"\n",
    "    if num == 0:\n",
    "        res = \"left\"\n",
    "    elif num == 1:\n",
    "        res = \"down\"\n",
    "    elif num == 3:\n",
    "        res = \"right\"\n",
    "    elif num == 4:\n",
    "        res = \"up\"\n",
    "    return res\n",
    "        \n",
    "\n",
    "for i in range(63):\n",
    "    print(\"Optimal policy for state \",i,\":\", getBestMoves(np.argmax(q_table[i,:])))\n",
    "\n",
    "# action = np.argmax(q_table[1,:]) \n",
    "# print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a9e03",
   "metadata": {},
   "source": [
    "These are the best move for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3590ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
